{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!python module_preprocessing.py --default=True --train_ann='/input/train_annotations_equal.json' --dataset_type=1 --test_ann='/input/test_annotations_equal.json' --train_split=0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detectron2 using cocolike structure training mask rcnn 50 layer.\n",
    "\n",
    "data: module_preprocessing.py\n",
    "data augmentations: https://jss367.github.io/Data-Augmentation-with-Detectron2.html / https://detectron2.readthedocs.io/modules/data_transforms.html\n",
    "- flip horisontal 50% prob\n",
    "- flip vertical 50% prob\n",
    "- random rotation -20 to 20%. \n",
    "- random lightning 0.1 standard deviations. \n",
    "\n",
    "3 Stages:\n",
    "- 1st stage  256x256 images\n",
    "- 2nd stage  512x512 images\n",
    "- 3rd stage  756x756 images\n",
    "- hopefully with variable optimized learning rate. \n",
    "\n",
    "Configurations have been changed from the default. Current notebook contains only the mask training part. Documentation for detectron2 documentation: https://detectron2.readthedocs.io/modules/config.html\n",
    "\n",
    "\n",
    "Metrics: tensorboard (on local: http://127.0.0.1:6006) -> call from another script/notebook.\n",
    "\n",
    "Submission: module_submittion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available(), torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import pycocotools\n",
    "import random \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "assert torch.__version__.startswith(\"1.6\")\n",
    "\n",
    "import detectron2\n",
    "import detectron2.data.transforms as T\n",
    "import detectron2.utils.comm as comm\n",
    "\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "from detectron2.data import MetadataCatalog,DatasetMapper,build_detection_train_loader,build_detection_test_loader\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.data.catalog import DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances \n",
    "\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "\n",
    "from detectron2.projects.deeplab import add_deeplab_config, build_lr_scheduler\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True #Truncated image -> https://github.com/keras-team/keras/issues/5475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.abspath(os.getcwd())\n",
    "\n",
    "register_coco_instances(\"my_dataset_train_v2\",{},PATH + \"/input/train_annotations_equal.json\",PATH + \"/input/train_v2/\")\n",
    "register_coco_instances(\"my_dataset_val_v2\",{},PATH + \"/input/test_annotations_equal.json\",PATH + \"/input/train_v2/\")\n",
    "\n",
    "my_dataset_train_metadata = MetadataCatalog.get(\"my_dataset_train_v2\")\n",
    "dataset_dicts = DatasetCatalog.get(\"my_dataset_train_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mapper(dataset_dict):\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "    # List of transforms https://detectron2.readthedocs.io/modules/data_transforms.html\n",
    "    # Add saturation, add shear orsmth.\n",
    "    transform_list = [\n",
    "                      T.RandomFlip(prob=0.5, horizontal=False, vertical=True),\n",
    "                      T.RandomFlip(prob=0.5, horizontal=True, vertical=False),\n",
    "                      T.RandomLighting(0.1),\n",
    "                      T.RandomRotation((-0.2,0.2))\n",
    "                     ]\n",
    "    image, transforms = T.apply_transform_gens(transform_list, image)\n",
    "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "\n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
    "        for obj in dataset_dict.pop(\"annotations\")\n",
    "        if obj.get(\"iscrowd\", 0) == 0\n",
    "    ]\n",
    "    instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
    "    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            os.makedirs(\"coco_eval\", exist_ok=True)\n",
    "            output_folder = \"coco_eval\"\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n",
    "    \n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_detection_train_loader(cfg, mapper=custom_mapper)\n",
    "    \n",
    "    @classmethod\n",
    "    def build_lr_scheduler(cls, cfg, optimizer):\n",
    "        return build_lr_scheduler(cfg, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call from anywhere else. \n",
    "#!tensorboard --logdir=run_equal --host=0.0.0.0\n",
    "#http://0.0.0.0:6006/#scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"my_dataset_train_v2\",) \n",
    "cfg.DATASETS.TEST = (\"my_dataset_val_v2\",)\n",
    "cfg.TEST.EVAL_PERIOD = 5000\n",
    "cfg.DATALOADER.NUM_WORKERS = 4 ## 4 per gpu\n",
    "cfg.SOLVER.IMS_PER_BATCH = 16\n",
    "cfg.SOLVER.BASE_LR = 0.001  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 20000\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128 \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Only has one class (ship)\n",
    "cfg.MAX_SIZE_TRAIN = 256 #Max image size \n",
    "cfg.OUTPUT_DIR = \"./runs/run_50_anchortest\"\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = True # Teach only ships & background\n",
    "cfg.LR_SCHEDULER_NAME = \"WarmupCosineLR\" #avoid getting stuck in local minima.\n",
    "cfg.CUDNN_BENCHMARK = True\n",
    "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[16, 32, 64, 128, 256, 512]]\n",
    "cfg.SOLVER.AMP.ENABLED = True  # Automatic Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = CocoTrainer(cfg) \n",
    "trainer.resume_or_load(resume=True) #True takes last checkpoint file which is saved below.\n",
    "trainer.train() #Trainer will throw out non-annotated pictures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"my_dataset_train_v2\",) \n",
    "cfg.DATASETS.TEST = (\"my_dataset_val_v2\",)\n",
    "cfg.TEST.EVAL_PERIOD = 5000\n",
    "cfg.DATALOADER.NUM_WORKERS = 4 ## 4 per gpu\n",
    "cfg.SOLVER.IMS_PER_BATCH = 12\n",
    "cfg.SOLVER.BASE_LR = 0.001  \n",
    "cfg.SOLVER.MAX_ITER = 30000\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256 \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ship)\n",
    "cfg.MAX_SIZE_TRAIN = 512 #Max image size \n",
    "cfg.LR_SCHEDULER_NAME = \"WarmupCosineLR\" #avoid getting stuck in local minima. \n",
    "cfg.OUTPUT_DIR = \"./runs/run_50_anchortest\"\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = True\n",
    "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[16, 32, 64, 128, 256, 512]]\n",
    "cfg.SOLVER.AMP.ENABLED = True  # Automatic Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = CocoTrainer(cfg) \n",
    "trainer.resume_or_load(resume=True) #True takes last checkpoint file which is saved below.\n",
    "trainer.train() #Trainer will throw out non-annotated pictures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"my_dataset_train_v2\",) \n",
    "cfg.DATASETS.TEST = (\"my_dataset_val_v2\",)\n",
    "cfg.TEST.EVAL_PERIOD = 5000\n",
    "cfg.DATALOADER.NUM_WORKERS = 4 ## 4 per gpu\n",
    "cfg.SOLVER.IMS_PER_BATCH = 10\n",
    "cfg.SOLVER.BASE_LR = 0.001  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 110000\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ship)\n",
    "cfg.MAX_SIZE_TRAIN = 756 #Max image size \n",
    "cfg.SOLVER.STEPS=(70000, 105000) #reduce gradually lr. \n",
    "cfg.OUTPUT_DIR = \"./runs/run_50_anchortest\"\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = True\n",
    "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[16, 32, 64, 128, 256, 512]]\n",
    "cfg.SOLVER.AMP.ENABLED = True  # Automatic Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = CocoTrainer(cfg) \n",
    "trainer.resume_or_load(resume=True) #True takes last checkpoint file which is saved below.\n",
    "trainer.train() #Trainer will throw out non-annotated pictures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a submission for kaggle. \n",
    "\n",
    "# There will be an overload error: https://github.com/pytorch/vision/pull/2705\n",
    "# With classifier predictions included\n",
    "!python module_submit.py --model_path=\"runs/run_50_anchortest\" --submit_csv=\"submit_50_anchortest_1.csv\" --score_thres=0.8 --ship_proba_csv=\"test_ship_proba.csv\" --anchor_sizes=\"small\"\n",
    "# By itself\n",
    "!python module_submit.py --model_path=\"runs/run_50_anchortest\" --submit_csv=\"submit_50_anchortest_2.csv\" --score_thres=0.8 --anchor_sizes=\"small\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
